<!DOCTYPE html>
<html  xml:lang="en" lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>GlassFish Wiki : GMS3.2WithoutMulticast</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="description" content="" />
        <meta http-equiv="content-language" content="en" />
        <link href="https://netdna.bootstrapcdn.com/twitter-bootstrap/2.2.2/css/bootstrap.min.css" rel="stylesheet" />
        <link href="https://netdna.bootstrapcdn.com/twitter-bootstrap/2.2.2/css/bootstrap-responsive.min.css" rel="stylesheet" />
        <link href="styles/docs.css" rel="stylesheet" />
        <link href="styles/site1.css" rel="stylesheet" />
        <style> a { color: #555555; } </style>
        <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
        <!--[if lt IE 9]>
                <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
    </head>

    <body class="page-documentation project-gfmvnsite" data-spy="scroll" data-offset="60" data-target="#toc-scroll-target">

        <div class="navbar navbar-fixed-top">
            <div class="navbar-inner">
                <div class="container">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <div class="brand"><a href="/glassfish/index.html"><img src="../images/gflogo24.png"><span style="color:#E88A43;font-size:18px;padding-left:11px;padding-top:15px;font-weight:bold;">GlassFish</span> - <span style="font-size:18px;" class="gf-grey">World's first Java EE 7 Application Server</span></a></div>
                    <div class="nav-collapse">
                        <ul class="nav pull-right">
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
									<li><a href="/glassfish/LICENSE" title="License">Legal </a></li>
									<li><a href="/glassfish/CONTRIBUTING" title="Contributing">Contributing </a></li>
                                </ul>
                            </li>
                        </ul>
                    </div><!--/.nav-collapse -->
                </div>
            </div>
        </div>

        <br/><br/>
        <div class="container"><body> 
 <table class="pagecontent" border="0" cellpadding="0" cellspacing="0" width="100%" bgcolor="#ffffff"> 
  <tbody>
   <tr> 
    <td valign="top" class="pagebody">   <h4><a name="GMS3.2WithoutMulticast-GlassFish3.1NextGMSNonMulticastDesign"></a>GlassFish 3.1 Next GMS Non-Multicast Design</h1> <p>Authors: Joe Fialli, Bobby Bissett</p> <p>Date: 07/27/2011: No longer for GlassFish 3.2, just calling GlassFish next. <br> Update to reflect deprecating VIRTUAL_MULTICAST_URI_LIST for gms over jxta only. Replaced with DISCOVERY_URI_LIST.</p> <p>Date: 04/15/2011</p> <h4><a name="GMS3.2WithoutMulticast-1.0Overview%3A"></a>1.0 Overview:</h2> <p>This document describes changes to support GMS in an environment without UDP multicast enabled between all clustered instances. It also discusses optimizations to enable scaling of the max number of instances in a cluster when in this mode. Lastly, it identifies changes to assist in decentralizing GMS operations away from the GMS Master in preparation to handle self-configuring cluster mode of running without a DAS. The DAS is isolated from application load on the cluster so it was beneficial to place a lot of GMS system level<br> processing on it. This processing was not impacted by high application load on the CORE clustered instance.</p> <h4><a name="GMS3.2WithoutMulticast-1.1Terminology"></a>1.1 Terminology</h2> <p>Shoal GMS is implemented in a GlassFish subproject and does have terminology that is more generic than its usage within GlassFish.</p> <p>This section provides a mapping between GlassFish clustering terms to Shoal GMS terms. When discussing GMS design within this document, the GMS terms, not the GlassFish instantiations of GMS concepts, is used. This section is to assist GlassFish reviewers in relating discussion to GlassFish.</p> <p>A <b><em>GlassFish cluster</em></b> is mapped to a <b><em>Shoal GMS Group</em></b>.<br> A <b><em>GlassFish clustered instance</em></b> is mapped to a <b><em>Shoal GMS group member</em></b>.<br> A <b><em>GlassFish DAS</em></b> is a member of all <b><em>gms-enabled GlassFish clusters</em></b> listed in its domain.xml configuration.<br> A <b><em>GlassFish DAS</em></b> is a <b><em>GMS Spectator</em></b>, an observer of the GMS Group. A GMS Spectator does not<br> provide a HA replication store and it is not considered as a place to replicate session data within a GlassFish cluster.<br> A <b><em>GlassFish DAS</em></b> is typically the <b><em>GMS Master</em></b>, but since the GMS Master can not be a single point of failure,<br> if the DAS leaves the GMS Group, another GlassFish clustered instance will assume the GMS Master role.<br> The <b><em>GMS Master</em></b> provides centralized processing so all GMS Members have same GMS member view (GlassFish clustered instances).</p> <h4><a name="GMS3.2WithoutMulticast-2.0DesignNotes"></a>2.0 Design Notes</h2> <h4><a name="GMS3.2WithoutMulticast-2.1.GMSConfigurationChangesinGlassFish3.1nextdomain.xml"></a>2.1. GMS Configuration Changes in GlassFish 3.1 next domain.xml</h3> <p>See <a href="20876192.html" title="GF+3.2+GMS+Configuration+in+domain.xml">GF 3.1 Next GMS Configuration in domain.xml </a></p> <p>interface Multicast Sender is implemented by</p> <p>1. com.sun.enterprise.mgmt.transport.VirtualMulticastSender when glassfish cluster/group-management-service property<br> GMS_DISCOVERY_URI_LIST is set.</p> <p>2. com.sun.enterprise.mgmt.transport.BlockingIOMulticastSender when multicast address and port are set.<br> (and DISCOVERY_URI_LIST is not set.)</p> <p>3. not supporting hybrid scenario where multicast address and port are set AND DISCOVERY_URI_LIST<br> is set. (proposed implementation simplification for time being. See GMS requirements doc non-requirements<br> GMS-1.1.3 and GMS-1.1.5. The implementation rationale for this requirement of not supporting hybrid is<br> that work would need to be done to ensure that a given message sent over both UDP multicast and<br> individually to each instance listed in DISCOVERY_URI_LIST was not delivered in duplicate.<br> There is currently no mechanism to ensure that a given GMS message is delivered at most one time.<br> We observed duplicate message delivery when VirtualMulticastSender was both delivering messages<br> over UDP multicast, when it was enabled, and unicast the same messge to each GMS member.</p> <p>Outstanding need from GlassFish 3.2 domain instance:<br> a globally unique id to use for namespace to evaluate cluster name within.<br> (one should be able to have two clusters with same name but the clusters are created in the<br> context of different domains.) If not possible, then<br> See Dependencies GMS-1.2.2 and GMS-1.2.3 in GMS Requirements document.</p> <p>ASARCH Review comments from May 10th<br> 1. requested considering gms tcp port to be port unified with asadmin port. (need to consult with Alexey)<br> must consider that gms is enabling user to enable ssl on gms tcp connection. Is it<br> possible to do port unification with asadmin with chance of ssl being enabled for gms?<br> 2. other option is to have a restful service that provides ports for services. IIOP and GMS could use this.</p> <p>3. file a RFE on enabling securing GMS heartbeat info sent over multicast. (next release)</p> <p>2.1.1 GMS Non-Multicast Implementation</p> <p>com.sun.enterprise.mgmt.transport.VirtualMulticastSender contains a list of all active members.</p> <p>This list is maintained by adding a member joining the group in<br> com.sun.enterprise.mgmt.transport.NetworkManager.addRemotePeer(PeerID)<br> and removing a member leaving the group when the method<br> com.sun.enterprise.mgmt.transport.NetworkManager.removePeerID(PeerID)<br> is called.</p> <p>The active member list is initialized with values from VIRTUAL_MULTICAST_URI_LIST.</p> <p>The VirtualMulticastSender.doBroadcast(Message msg) iterates over all active<br> members of the gms group, sending the msg to them each member over unicast.<br> There is a proposal to enable configuration that some messages be able to <br> be sent over lower overhead UDP unicast rather than higher overhead TCP unicast.<br> The GMS member heartbeat is an initial candidate for this. </p> <h4><a name="GMS3.2WithoutMulticast-2.2HeartBeatFailureDetection"></a>2.2 HeartBeat Failure Detection</h3> <p>The current heartbeat failure detection implementation is optimized for the<br> heartbeats to be sent via UDP multicast. The majority of the HealthMonitoring<br> failure detection is centralized in GMS Master. It is the only instance that monitors<br> all other instances heartbeats.<br> It was desirable in GlassFish 2.x and GlassFish 3.1 for this centralized processing<br> to occur in GMS master which typically would be DAS. (domain server)<br> This isolated GMS health monitoring processing from the application load on the<br> cluster. The DAS was not a single point of failure as default GMS master.<br> If DAS was stopped or failed, the other members in cluster would identify this<br> and elect a successor GMS Master. (The GMS notification GroupLeadershipNotification<br> represents a change in GMS Master for the GMS group. A GMS group maps to a<br> glassfish cluster).</p> <p>GlassFish 3.1 Next will be targeted to run in configurations/environments where UDP multicast<br> is not be available and the DAS may not be running(see ad hoc clustering). Thus, we will introduce<br> an alternative heartbeat failure detection algorithm that decentralizes processing<br> some and is optimized for unicast heartbeats.</p> <p>2.2.1 GMS Heartbeat Failure Detection in GlassFish 3.1</p> <p>The current heartbeat failure detection algorithm relies on UDP multicast transport.<br> -Each cluster member broadcasts its heartbeat to all other members.<br> -Each cluster member records heartbeats for all other members. (timestamps receive time)</p> 
     <ul class="alternate" type="square"> 
      <li>Each cluster member cleans its connection caches to a member when it receives a health message of<br> a PLANNED_SHUTDOWN or FAILURE about that member.<br> -Only the master member detects, validates and notifies all other members of a suspected or failed instance.<br> -All members in cluster monitor master's heartbeat. If an instance detects that master has<br> failed, it uses algorithm shared by all members to compute the new master. Only instance that<br> is appointed new master by algorithm will send notification to other cluster members that<br> it is new Master (via GroupLeadershipNotication) and then that the former master has failed.</li> 
     </ul> <p>A health message is of type HEALTH_MESSAGE and is composed of<br> HealthMessage.Entry which has a health state and system advertisement<br> (the gms client start time in sys advertisement is instrumental in<br> detecting instances that have restarted faster than heartbeat failure<br> detection could detect that previous instantiation of instance has<br> failed and was restarted). When the health message is received, the<br> HealthMonitor.receiveMessageEvent timestamps the local receive time on<br> the HealthMessage.Entry and places this entry in the HealthMonitor map<br> of clustered instance peerid to last HealthMessage.Entry received.<br> The health message may be the first message received by an instance<br> from another instance, so an instance should send its first heartbeat<br> to all instances. There is dynamic contact information in the message<br> that is cached and used to send messages back to that instance. All<br> messages from that instance contain this contact info. However, the<br> heartbeat message is typically a candidate for being the first message<br> received from a newly joined member of the cluster that assists in<br> filling the dynamic routing cache with necessary info to send GMS<br> messages back to the newly joined instance.</p> <p>When DISCOVERY_URI_LIST is enabled, the virtual<br> multicast sender over TCP is currently working. But as the number of<br> instances in the cluster increase, the overhead related to simulating<br> UDP multicast by sending a heartbeat to each instance over unicast<br> will increase overhead, particularly in the GMS Master which for<br> self-configuring clusters running without a DAS, one of the clustered<br> CORE instances would be performing GMS system overhead and handling<br> application requests.</p> <p>Lastly, each heartbeat from the master also includes the last master<br> sequence id sent by the master. Each instance inspects the sequence id and<br> checks if it has received all prior GMS notifications sent by the<br> master. If there are any missing GMS notification from the master,<br> the instance requests that the master rebroadcast the missed messages<br> to it. This was a bug fix in GF 3.1 for dropped UDP messages.</p> <p>2.2.2 Lower Risk Incremental Optimizations to existing HealthMonitor</p> 
     <ul class="alternate" type="square"> 
      <li>Provide a capability to configure UDP unicast to be used for heartbeat and/or GMS notifications.</li> 
      <li>Requires an ability to configure UDP unicast port and DatagramSocket.</li> 
      <li>Disclaimer: s</li> 
     </ul> 
     <ul class="alternate" type="square"> 
      <li>Incrementally Evolved Heartbeat failure detection algorithm</li> 
     </ul> <p>Assume a cluster of N members.</p> 
     <ul class="alternate" type="square"> 
      <li>Each cluster member sends a ALIVE and ALIVEANDREADY heartbeat only to master.<br> (savings of (N-1)^2 unicast heartbeat messages per heartbeat frequency from other instances to master)</li> 
      <li>Master sends a heartbeat to X mostly likely to be selected master members in cluster. (could default to all members)<br> (savings of (N-1) - X unicast heartbeat messages per heartbeat frequency from master to cluster)</li> 
      <li>Due to savings on unicast heartbeat monitor, when there is a change in master, initialize heartbeat monitor<br> to all existing members with a TCP ping to ensure that it does not take longer to detect instances that fail<br> at same time or close to same time as master failed. (shorten window of possible missed failures due to a change<br> in GMS Master).</li> 
      <li>When master performs planned shutdown, master sends its heartbeat info to master candidate.</li> 
      <li>Instances flush connection cache when receive PLANNED_SHUTDOWN or FAILURE notification from master.</li> 
     </ul> <p>2.2.3 New Heartbeat Failure Detection algorithm</p> <p>This new algorithm will be optimized for no UDP multicast and no DAS running.<br> (These conditions are derived from PRD CLUST-2 Ad Hoc Clustering Support)<br> It will decentralize more of the failure detection processing from GMS Master<br> while at same time preserving the consistent view across all cluster by having<br> a Master (also known as a GroupLeader) that is only instance to send<br> GMS Views around.</p> <p>Neighbor heartbeat failure detection decentralizes initial identification that<br> an instance is potentially suspected of failure. Rather than master watching<br> all other instances in the cluster, each clustered instance is responsible for<br> monitoring that it is receiving its neighbor's periodic heartbeat. Additionally,<br> an optimization can be added that if a GMS message is sent to a neighbor, the<br> receiver will treat the received message as proof that the instance is still<br> working and will infer a ALIVE/ALIVEANDREADY heartbeat message.</p> <p>For all the members of a GMS Group, GMS defines an ordering of these members based on<br> a sorting of the PeerID object. Each member of the GMS group has the same view of<br> the active members of the GMS Group. This ordering is already used to calculate<br> the replacement Master when the current GMS master leaves the GMS group.<br> Now this ordering will be used to calculate whether an instance is a neighbor of another<br> instance in the group.</p> <p>2.2.3.1 Definitions:<br> Given a GlassFish 3.1 Next cluster with N members and a DAS, there will be N+1 GMS group members.<br> Given a GlassFish 3.1 Next cluster with N members and no DAS, there will be N GMS group members.</p> <p>To normalize between above to scenarios, lets just consider that the GMS group has M members for<br> these definitions. There exist a well defined ordering of these members and we will refer<br> to these members with notation of m1 to mM.</p> <p>For n between 1..M members,<br> mn sends its health messages to neighbor mn+1, mM sends its health message to m1.<br> mn+1 is considered the neighbor health monitor for member mn.<br> when n + 1 is M, m1 is the neighbor health monitor for member mM.</p> <p>2.2.3.2 Boundary cases for neighbor heartbeat failure detection:<br> These occur when group membership is changing quickly.</p> <p>1. cluster startup time.<br> Multiple instances joining at same time. Since "asadmin<br> start-cluster" is a non-requirement for self-configuring clusters<br> the GMS optimization to notify GMS clients whether an instance is<br> joining as part of group startup or an isolated instance startup<br> will not exist. All instance startups will be considered a single<br> instance starting up. The neighbor heartbeat failure detection<br> algorithm will not work as quickly when multiple instances are<br> joining or leaving at same time. (in configurations that asadmin<br> subcommands start-cluster &amp; stop-cluster are supported GMS<br> notifications have a subevent to indicate whether group or just<br> individual instances are changing their group membership.)<br> Neighbors will be constantly changing as instances start up. The<br> actual time between each instance starting for a self-configuring<br> cluster with a minimum of instances over 2 will be OS, VM, GF 3.1 Next<br> implementation dependent and the timings will change as we progress<br> through GF 3.1 Next. (Experience in GF 3.1, when "start-cluster" went<br> from serialized starting to parallel starting, the change in timing<br> uncovered other issues)</p> <p>2. fast restart - when a clustered instance fails and the same identified<br> instance is quickly restarted.<br> In GlassFish 2.x, the nodeagent watched and restarted failed<br> instances. In GlassFish 3.1, one had an option of installing an<br> instance as an OS service. Thus, the OS would monitor process and<br> restart the service if it had a software failure (such as out of<br> memory, segv, ....) Also, for this boundary case, rejoin detection<br> requires the neighbor to detect the restarted instance and add a<br> rejoin subevent to the GMS notification of JOIN and<br> JOINED_AND_READY.</p> <p>3. instances considered to be neighbors failing at same time.<br> There will be a delay in detecting failure of the instance that fails<br> whose neighbor who was watching it also fails.</p> <p>2.2.3.3 Description of inferred heartbeat from a GMS message.</p> <p>Sending a GMS message that is not a HEALTH_MESSAGE to neighbor monitoring instance:<br> 1. when an instance sends a GMS message to its heartbeat neighbor,<br> it checks to see if its current HealthMonitor state is ALIVE or ALIVEANDREADY.<br> For either of these states, the sending instance sets a last heartbeat timestamp to that<br> neighbor to the current system time. (Thus, avoiding the next heartbeat message to that instance)</p> <p>Neighbor health monitor receiving a GMS message from its neighbor.<br> 1. when receiving a GMS message that is not a HEALTH_MESSAGE type.<br> Check if the message sender is considered a neighbor.<br> If so, then check if last heartbeat from that neighbor was<br> ALIVE or ALIVEANDREADY. If so, treat this received message as<br> inferred HealthMessage with the last received HEALTH_MESSAGE state.<br> Next check if NODEADV confirms that<br> the sending instance is the same invocation of this instance<br> as the last heartbeat was. (to detect fast restart case when<br> they do not match) If from same invocation of the GMS client,<br> then set timestamp of HealthMessage.Entry to have current<br> system time receive</p> <h4><a name="GMS3.2WithoutMulticast-2.3.VirtualMulticastOptimizations"></a>2.3. Virtual Multicast Optimizations</h3> <p>Send in parallel threads over unicast to each cluster member.<br> Only deserialize GMS message once and share one NIO buffer (via different views)<br> to write bytes of serialized message via unicast messaging to each<br> member of the cluster.</p> <h4><a name="GMS3.2WithoutMulticast-2.4GMSMaster%28GroupLeadershipNotification%29"></a>2.4 GMS Master (GroupLeadershipNotification)</h3> <p>The first clustered instance started for a given cluster will be the GMS master for the cluster.<br> The GMS Notification GroupLeadershipNotification is sent to all clustered instances when<br> an instance is made the GMS Master.</p> <p>Rather than a requirement, the GMS module would strongly recommend that when the elasticity<br> manager needs to stop an instance in the cluster, that preference would be given to NOT<br> stop the current GMS master. While the GMS master is not a single point of failure, it would<br> improve the continuity of GMS processing if the GMS Master is not constantly changing.</p> <p>Worst case scenario would be that GMS had an algorithm that the longest running member<br> becomes the new Master and the elasticity manager when it is shrinking the cluster<br> has a preference to stop the longest running member. These two rules would<br> result in churning of the GMS Master. (leading to an excess of GroupLeadershipNotification<br> to the other clustered instances.)</p> </td> 
   </tr> 
  </tbody>
 </table>    
</body></div>
        <br/>

        <!-- footer================================================== -->
        <footer class="well">
            <div class="container">

                <div class="row-fluid" id="bottom-info">
                    <!--div class="span6 pagination-centered" id="social"-->
                    <div class="span4" id="social">			
                        <a href="http://blogs.oracle.com/theaquarium/"><img src="../images/icons/TheAquarium.png"></a>
                        <a href="https://twitter.com/glassfish"><img src="../images/icons/twitter.png"></a>
                        <a href="https://plus.google.com/communities/106098646151660933759"><img src="../images/icons/google.png"></a>
                        <a href="http://www.linkedin.com/groups/GlassFish-Users-106819/about"><img src="../images/icons/linkedin.png"></a>
                        <a href="http://www.youtube.com/user/GlassFishVideos"><img src="../images/icons/youtube.png"></a>
                        <a href="https://www.facebook.com/GlassFish"><img src="../images/icons/facebook.png"></a>
                    </div>

                    <div class="span8" id="copyright">Page last changed on Jul 27, 2011 by 
<font color="#0050B2">jfialli</font>. Exported from wikis.oracle.com on May 27, 2015 20:47.<br/>
                        Copyright &copy; 2005-2015 Oracle Corporation and/or its affiliates.</div>
                </div>
            </div>
        </footer>

        <!-- ================================================== -->
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.1/jquery.min.js"></script>
	<script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/bootstrap-tab.js"></script>
	<script src="https://netdna.bootstrapcdn.com/twitter-bootstrap/2.2.2/js/bootstrap.min.js"></script>
	<!--  Begin SiteCatalyst code  -->
  	<!--  End SiteCatalyst code  -->
    </body>
</html>